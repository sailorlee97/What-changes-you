# ğŸ¦œ Multi-ARCL: Multimodal Adaptive Relay-based Distributed Continual Learning for Encrypted Traffic Classification 

<p align="center">
  <a href="#-introduction">ğŸ‰Introduction</a> â€¢
  <a href="#-model">ğŸ¦œModel</a> â€¢
  <a href="#-train">ğŸ”¥Train</a> â€¢
  <a href="#-datasets">ğŸŒŸDatasets</a> â€¢
  <a href="#-mmmb">ğŸ„MMMB</a> <br />
  <a href="#-quick-start">ğŸ“Quick Start</a> â€¢
  <a href="#-acknowledgement">ğŸ‘¨â€ğŸ«Acknowledgement</a> â€¢  
  <a href="#-contact">ğŸ¤—Contact</a>
</p>

## ğŸ‰ Introduction

## ğŸ¦œ Model
<div align="center">
  <img src="./images/workflow.png" width="800px" />
</div>

## ğŸ”¥ Train

if you use parallel computing, please use this codeï¼š
two gpus
```
CUDA_VISIBLE_DEVICES=0,1 python -m torch.distributed.launch --nproc_per_node=2 secondmain.py
```
four gpus
```
CUDA_VISIBLE_DEVICES=0,1,2,3 python -m torch.distributed.launch --nproc_per_node=4 secondmain.py
```
if you find the error: 
```
File "/home/vipuser/miniconda3/lib/python3.8/site-packages/torch/distributed/rendezvous.py", line 179, in _env_rendezvous_handler
    store = TCPStore(master_addr, master_port, world_size, start_daemon, timeout)
RuntimeError: Address already in use
...
```
use this command:
```
kill $(lsof -t -i:12359)
```
and then reuse parallel computing.
